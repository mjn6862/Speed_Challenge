{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Matthew categorical edit Speed Challenge Framework",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjn6862/Speed_Challenge/blob/master/Matthew_categorical_Speed_Challenge_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTMpzey5z5c",
        "colab_type": "text"
      },
      "source": [
        "**Comma AI Speed Challenge**\n",
        "\n",
        "  This notebook will contain (hopefully) all of the functions you need to import the data into your model.\n",
        "\n",
        "  ***Be sure to train with GPU acceleration enabled***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kq-rlJxzHbr",
        "colab_type": "text"
      },
      "source": [
        "**Import Statements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sguc70jY5xt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from datetime import datetime\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "458i7-V-qmEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNzSFHCVDKGp",
        "colab_type": "text"
      },
      "source": [
        "**Custom Data Generator**\n",
        "\n",
        "This works (I think) for giving two sequential images to a Keras Functional model as well as the velocity associated with the second image.\n",
        "\n",
        "At this point, don't worry about how this works. If you need something changed or fixed, just ask. This is the boring part anyways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YTEHHM99Jhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max speed is 28.130404\n",
        "# bucket speed into 15 2-mph bins for categorical data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXXe8EiADaTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to mount drive before using this\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
        "                 n_classes=10, shuffle=True, categorical=False):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.categorical = categorical # my edition\n",
        "        self.on_epoch_end()\n",
        "        self.direct = \"./drive/My Drive/commai_dataset/\"\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        #return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "        return len(self.list_IDs)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        \n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = self.list_IDs[index]\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.load(self.direct+\"data/data_\" + list_IDs_temp +\".npy\")\n",
        "        # normalize: my edition\n",
        "        X = X/255\n",
        "        x1 = X[0:101,:,:,:]\n",
        "        x2 = X[1:102,:,:,:]\n",
        "        y = np.load(self.direct+\"labels/label_\" + list_IDs_temp +\".npy\")\n",
        "        y = y[1:]\n",
        "        if self.categorical == True: # my edition\n",
        "          y = np.array([spd//2 for spd in y],dtype=int)\n",
        "\n",
        "        return [x1, x2], y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJy-HdjtEH-o",
        "colab_type": "text"
      },
      "source": [
        "**Define custom loss function**\n",
        "\n",
        "This is not well tested, neither is it optimized. You might not even want to use this function.\n",
        "\n",
        "Keras backend functions are a powerful tool for writing custom loss functions. To define a loss function it just has to accept *y_true* and *y_pred* as arguments and return a float.\n",
        "\n",
        "To use your new loss function, change the argument in *model.compile()*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raqNfdCNENGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sum_sq_err(y_true, y_pred):\n",
        "    return tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgc5HQbfFvN1",
        "colab_type": "text"
      },
      "source": [
        "**Define the test-train split and create the Data Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru6ugzA6Fzc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'dim': (110,320),\n",
        "          'batch_size': 101,\n",
        "          'n_classes': 1,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': False,\n",
        "          'categorical': True}\n",
        "\n",
        "train_data = []\n",
        "train_label = []\n",
        "valid_data = []\n",
        "valid_label = []\n",
        "\n",
        "for i in range(70):\n",
        "    train_data.append(\"%03d\" %i)\n",
        "    train_data.append(\"%03d\" %(i+100))\n",
        "\n",
        "for i in range(70, 100):\n",
        "    valid_data.append(\"%03d\" %i)\n",
        "    valid_data.append(\"%03d\" %(i+100))\n",
        "\n",
        "\n",
        "\n",
        "partition={'train':train_data, 'validation':valid_data}\n",
        "labels = {'train': train_label,'validation':valid_label}\n",
        "\n",
        "\n",
        "cat_train_generator = DataGenerator(partition['train'], labels['train'], **params)\n",
        "cat_val_generator = DataGenerator(partition['validation'], labels['validation'], **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D0xuO74GB9N",
        "colab_type": "text"
      },
      "source": [
        "**Define the input layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcU50BoNsbMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, concatenate, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNebndf-rqCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputA = Input(shape=(110,320,3), name='first_image') \n",
        "inputB = Input(shape=(110,320,3), name='second_image') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfgLVnl4GHC1",
        "colab_type": "text"
      },
      "source": [
        "**Define the model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YljC2LhSr9H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convA = Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu', name='convA')(inputA)\n",
        "dropA = Dropout(0.1, name='dropA')(convA)\n",
        "\n",
        "convB = Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu', name='convB')(inputB)\n",
        "dropB = Dropout(0.1, name='dropB')(convB)\n",
        "\n",
        "conc = concatenate(inputs=[dropA,dropB],name='conc')\n",
        "conv1 = Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu', name='conv1')(conc)\n",
        "drop1 = Dropout(0.1,name='drop1')(conv1)\n",
        "conv2 = Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu', name='conv2')(drop1)\n",
        "drop2 = Dropout(0.1, name='drop2')(conv2)\n",
        "#conv3 = Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu', name='conv3')(conv2)\n",
        "\n",
        "flat = Flatten(name='flat')(drop2)\n",
        "\n",
        "fc1 = Dense(128, activation='relu', name='fc1')(flat)\n",
        "drop3 = Dropout(0.1, name='drop3')(fc1)\n",
        "\n",
        "fc2 = Dense(128, activation='relu', name='fc2')(drop3)\n",
        "drop4 = Dropout(0.1, name='drop4')(fc2)\n",
        "\n",
        "fc3 = Dense(128, activation='relu', name='fc3')(drop4)\n",
        "drop5 = Dropout(0.1, name='drop5')(fc3)\n",
        "\n",
        "out = Dense(15, name='out')(drop5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_jXpaQawLYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_model = tf.keras.Model(inputs=[inputA,inputB], outputs=out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orwQgmCpwga9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyKXx_R9GgXH",
        "colab_type": "text"
      },
      "source": [
        "**Declare the optimizer and loss function, then compile your *less ridiculous*  model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5wAeSgj19df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZlP1XIJq7gf",
        "colab_type": "text"
      },
      "source": [
        "**Prepare the TensorBoard and Callbacks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dn9Wd8Qq5aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logdir = os.path.join(\"speed_logs2\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "earlystop_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "keep = 30 # how many epochs to keep 0.001 as the learning rate\n",
        "def scheduler(epoch):\n",
        "  if epoch < keep:\n",
        "    return 0.0005\n",
        "  else:\n",
        "    return 0.0005*tf.math.exp(0.1*(keep-epoch))\n",
        "\n",
        "schedule_callback = keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6zUadpkHe9j",
        "colab_type": "text"
      },
      "source": [
        "**Train using the fit_generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRoFCuJx0Y7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_model.fit_generator(generator=cat_train_generator,\n",
        "                    validation_data=cat_val_generator,\n",
        "                    verbose=1, \n",
        "                    epochs=30,  \n",
        "          callbacks=[tensorboard_callback, \n",
        "                               earlystop_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dnQHt_erVcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir \\speed_logs2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf-S_rLY5ikz",
        "colab_type": "text"
      },
      "source": [
        "**Visualize the predictions**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Pl0M7E0Vf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = cat_model.predict(cat_val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0CHXKDLL4HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert outputs from logits to probability using softmax\n",
        "smax = tf.keras.layers.Softmax(name='smax')(out)\n",
        "prob_model = tf.keras.Model(inputs=[inputA,inputB],outputs=smax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vRHCh3sMa5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get softmax predictions\n",
        "predictions = prob_model.predict(cat_val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbK6kRdeN0RH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert to categorical predictions\n",
        "cat_preds = np.array([np.argmax(pred) for pred in predictions])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx2NZOfAxxAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get all validation labels from the generator into one array\n",
        "cat_val_labels = np.zeros(cat_preds.shape, dtype=int)\n",
        "for i in range(60):\n",
        "  cat_val_labels[i*101:(i+1)*101] = cat_val_generator[i][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUWVerUauzjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4b_IF0NRsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get a list containing arrays of predictions, one array for each class\n",
        "sorted_predictions = []\n",
        "for i in range(15):\n",
        "  preds = cat_preds[np.where(cat_val_labels==i)]\n",
        "  sorted_predictions.append(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov1IH9KQPl0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sort_bins = np.arange(0,30,1) # max label is 26.05\n",
        "pred_bins = np.arange(0,int(max(cat_preds))+1,1)\n",
        "label_bins = np.arange(0,2*int(max(cat_preds))+2,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bDsBj_fvFsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use histograms to see how well each size mph bin was predicted\n",
        "num_rows = 8\n",
        "num_cols = 2\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(4*2*num_cols, 4*num_rows))\n",
        "for i in range(len(sorted_predictions)-1):\n",
        "  plt.subplot(num_rows,2*num_cols, 2*i+1)\n",
        "  plt.hist(sorted_predictions[i], bins = pred_bins)\n",
        "  plt.xticks(ticks=pred_bins,labels=label_bins, rotation=-30)\n",
        "  plt.title('{}-{} mph'.format(2*sort_bins[i],2*sort_bins[i+1]))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyPMkEaQGYTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_model.compile(optimizer=keras.optimizers.Adam(),\n",
        "               loss='mean_squared_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG4Abz30GbtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_model.evaluate(validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}